## All About Big Data



### 1. What are 5 V's of data?
 
The 5 V's of Big Data are a set of characteristics that describe the challenges and dimensions of big data. 
The 5 V's are:

* **Volume:** 
Refers to the sheer size of the data being generated and collected. 
Big data involves datasets that are too large to be comfortably processed by traditional database systems. 
It often involves terabytes, petabytes, or even larger quantities of data.

* **Velocity:**
Describes the speed at which data is being generated, collected, and processed. 
With the proliferation of real-time systems, IoT devices, and social media, data can be generated at an unprecedented speed. 
This requires the ability to process and analyze data streams in near real-time.

* **Variety:** 
Represents the diverse types of data that are being generated. 
Data comes in many formats, including structured data (like databases), unstructured data (like text or images), 
and semi-structured data (like JSON or XML). 
Big data systems need to be able to handle this variety efficiently.

* **Variability:** 
Refers to the inconsistency in the data's format, quality, and meaning. 
Data sources might have different structures, missing values, or inaccuracies. 
Big data systems must be able to handle and process such variability effectively.

* **Veracity:** 
Relates to the trustworthiness and reliability of the data. 
With the large volume of data being generated from multiple sources, ensuring data quality and accuracy becomes a significant challenge. 
It's important to have mechanisms in place to validate and ensure the correctness of the data.

Some of the other important V's include: 

* **Value:** 
Highlighting the importance of deriving meaningful insights and value from the data. 
Collecting and processing data is valuable only if it leads to actionable insights and informed decision-making.

* **Validity:** 
Emphasizing the necessity of ensuring that the data is valid, accurate, and trustworthy.

* **Volatility:** 
Referring to the temporary nature of certain data streams or the rapid changes in data over time.
-------------------------------------------------
### 2. What is big data?
Big data refers to extremely large and complex datasets that cannot be easily managed, processed, or analyzed using traditional data processing tools and methods. These datasets are characterized by their volume, variety, velocity, and sometimes veracity.

The concept of big data has gained prominence due to advancements in technology, such as increased storage capacities, faster processing capabilities, and improved data analytics techniques. Organizations and researchers use big data analytics to extract valuable insights, trends, patterns, and correlations from these massive datasets. These insights can help with making informed decisions, improving business strategies, enhancing scientific research, and more.

To process and analyze big data effectively, specialized tools and technologies have emerged, such as distributed computing frameworks (like Hadoop), NoSQL databases, data lakes, and machine learning algorithms. These tools enable organizations to handle the challenges posed by the enormous amounts of data generated in today's digital world.


-------------------------------------------------
### 3. What is Apache Hadoop? (version details summary)
Apache Hadoop is an open-source framework designed for storing and processing large datasets in a distributed computing environment. It was created to address the challenges of handling big data, which involves massive volumes of data that cannot be easily managed or processed using traditional methods.


The core components of Apache Hadoop include:
* **Hadoop Distributed File System (HDFS):**
HDFS is a distributed file system that allows large datasets to be stored across multiple servers (nodes) in a cluster. It divides files into smaller blocks and replicates them across different nodes for fault tolerance. This architecture enables efficient data storage and retrieval.

* **MapReduce:**
MapReduce is a programming model and processing engine that allows developers to write programs for processing and analyzing large datasets in parallel across a distributed cluster. It simplifies the process of breaking down complex tasks into smaller tasks that can be executed concurrently.

* **YARN (Yet Another Resource Negotiator)**
YARN is a resource management and job scheduling component in Hadoop. It manages and allocates resources to different applications running on the cluster, enabling efficient utilization of cluster resources.

* **Hadoop Common:**
This includes libraries, utilities, and APIs used by various Hadoop modules. It provides a consistent and shared infrastructure for Hadoop components.

* **Hadoop Ecosystem**
Apart from the core components, the Hadoop ecosystem consists of various complementary projects and tools that extend the capabilities of Hadoop. These include tools for data ingestion (Apache Flume, Apache Kafka), data processing (Apache Pig, Apache Hive), data querying (Apache Drill, Apache Impala), and more.

-------------------------------------------------
4. What is GFS - google file system(download and study)white paper

-------------------------------------------------
5. What is YARN in hadoop context?

-------------------------------------------------
6. What is MapReduce ? Solve an example also.

-------------------------------------------------
7. What is Apache Tez?

-------------------------------------------------
8. What is Apache Spark?

-------------------------------------------------
9. Compare MapReduce / Tez / Spark 

-------------------------------------------------
10. What is PIG in hadoop context?

-------------------------------------------------
11. What is Sentry?

-------------------------------------------------
12. What is ZOOKEEPER in hadoop context?

-------------------------------------------------
13. What is Spark MLlib?

-------------------------------------------------

